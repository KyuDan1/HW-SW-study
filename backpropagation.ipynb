{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Forward Hook] Linear(in_features=4, out_features=3, bias=True)의 가중치:\n",
      "tensor([[-0.0037,  0.2682, -0.4115, -0.3680],\n",
      "        [-0.1926,  0.1341, -0.0099,  0.3964],\n",
      "        [-0.0444,  0.1323, -0.1511, -0.0983]])\n",
      "[Forward Hook] Linear(in_features=4, out_features=3, bias=True)의 편향:\n",
      "tensor([-0.4777, -0.3311, -0.2061])\n",
      "------------------------------------------------\n",
      "[Forward Hook] Linear(in_features=3, out_features=2, bias=True)의 가중치:\n",
      "tensor([[ 0.0214,  0.2282,  0.3464],\n",
      "        [-0.3914, -0.2514,  0.2097]])\n",
      "[Forward Hook] Linear(in_features=3, out_features=2, bias=True)의 편향:\n",
      "tensor([ 0.4794, -0.1188])\n",
      "------------------------------------------------\n",
      "=== PyTorch로 업데이트된 최종 파라미터 ===\n",
      "fc1.weight:\n",
      " tensor([[-0.0037,  0.2682, -0.4115, -0.3680],\n",
      "        [-0.1416,  0.1264, -0.0283,  0.4303],\n",
      "        [-0.0444,  0.1323, -0.1511, -0.0983]])\n",
      "fc1.bias:\n",
      " tensor([-0.4777, -0.3446, -0.2061])\n",
      "fc2.weight:\n",
      " tensor([[ 0.0214,  0.2695,  0.3464],\n",
      "        [-0.3914, -0.2426,  0.2097]])\n",
      "fc2.bias:\n",
      " tensor([ 0.5239, -0.0251])\n",
      "==================================================\n",
      "\n",
      "=== 수동 연산으로 업데이트된 최종 파라미터 ===\n",
      "W1:\n",
      " tensor([[-0.0037,  0.2682, -0.4115, -0.3680],\n",
      "        [-0.1416,  0.1264, -0.0283,  0.4303],\n",
      "        [-0.0444,  0.1323, -0.1511, -0.0983]])\n",
      "b1:\n",
      " tensor([-0.4777, -0.3446, -0.2061])\n",
      "W2:\n",
      " tensor([[ 0.0214,  0.2695,  0.3464],\n",
      "        [-0.3914, -0.2426,  0.2097]])\n",
      "b2:\n",
      " tensor([ 0.5239, -0.0251])\n",
      "==================================================\n",
      "\n",
      "=== PyTorch vs Manual 최종 파라미터 차이(절댓값 최댓값) ===\n",
      "fc1.weight 차이: 0.0\n",
      "fc1.bias   차이: 0.0\n",
      "fc2.weight 차이: 0.0\n",
      "fc2.bias   차이: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 동일한 난수 시드\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# =====================================\n",
    "# 1. 간단한 PyTorch 모델 & 초기 파라미터\n",
    "# =====================================\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 3)  # in=4, out=3\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(3, 2)  # in=3, out=2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleMLP()\n",
    "\n",
    "# Hook으로 forward 시점의 파라미터를 확인해봅시다.\n",
    "def weight_hook(module, input, output):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        print(f\"[Forward Hook] {module}의 가중치:\")\n",
    "        print(module.weight.data)\n",
    "        print(f\"[Forward Hook] {module}의 편향:\")\n",
    "        print(module.bias.data)\n",
    "        print(\"------------------------------------------------\")\n",
    "\n",
    "model.fc1.register_forward_hook(weight_hook)\n",
    "model.fc2.register_forward_hook(weight_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Forward Hook] Linear(in_features=4, out_features=3, bias=True)의 가중치:\n",
      "tensor([[-0.0037,  0.2682, -0.4115, -0.3680],\n",
      "        [-0.1416,  0.1264, -0.0283,  0.4303],\n",
      "        [-0.0444,  0.1323, -0.1511, -0.0983]])\n",
      "[Forward Hook] Linear(in_features=4, out_features=3, bias=True)의 편향:\n",
      "tensor([-0.4777, -0.3446, -0.2061])\n",
      "------------------------------------------------\n",
      "[Forward Hook] Linear(in_features=3, out_features=2, bias=True)의 가중치:\n",
      "tensor([[ 0.0214,  0.2695,  0.3464],\n",
      "        [-0.3914, -0.2426,  0.2097]])\n",
      "[Forward Hook] Linear(in_features=3, out_features=2, bias=True)의 편향:\n",
      "tensor([ 0.5725, -0.0086])\n",
      "------------------------------------------------\n",
      "=== PyTorch로 업데이트된 최종 파라미터 ===\n",
      "fc1.weight:\n",
      " tensor([[ 0.0171,  0.2728, -0.3823, -0.3625],\n",
      "        [-0.1416,  0.1264, -0.0283,  0.4303],\n",
      "        [-0.0572,  0.1295, -0.1692, -0.1017]])\n",
      "fc1.bias:\n",
      " tensor([-0.5022, -0.3446, -0.1910])\n",
      "fc2.weight:\n",
      " tensor([[ 0.0217,  0.2695,  0.3465],\n",
      "        [-0.3883, -0.2426,  0.2103]])\n",
      "fc2.bias:\n",
      " tensor([0.6073, 0.0631])\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 2. 예시용 입력 x, 타깃 y (batch=2, output=2)\n",
    "# ============================================\n",
    "x = torch.randn(2, 4)\n",
    "y = torch.randn(2, 2)\n",
    "\n",
    "# ==============================================\n",
    "# 3. PyTorch forward/backward/step 한 번 수행\n",
    "# ==============================================\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 파라미터 업데이트 전(초기) 파라미터를 복사해두기\n",
    "# --> 이 값을 가지고 수동 연산과 비교할 것\n",
    "init_W1 = model.fc1.weight.detach().clone()\n",
    "init_b1 = model.fc1.bias.detach().clone()\n",
    "init_W2 = model.fc2.weight.detach().clone()\n",
    "init_b2 = model.fc2.bias.detach().clone()\n",
    "\n",
    "# --- forward ---\n",
    "pred = model(x)\n",
    "loss = criterion(pred, y)\n",
    "\n",
    "# --- backward ---\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# --- update ---\n",
    "optimizer.step()\n",
    "\n",
    "print(\"=== PyTorch로 업데이트된 최종 파라미터 ===\")\n",
    "print(\"fc1.weight:\\n\", model.fc1.weight.data)\n",
    "print(\"fc1.bias:\\n\", model.fc1.bias.data)\n",
    "print(\"fc2.weight:\\n\", model.fc2.weight.data)\n",
    "print(\"fc2.bias:\\n\", model.fc2.bias.data)\n",
    "print(\"==================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 수동 연산으로 업데이트된 최종 파라미터 ===\n",
      "W1:\n",
      " tensor([[ 0.0171,  0.2728, -0.3823, -0.3625],\n",
      "        [-0.1416,  0.1264, -0.0283,  0.4303],\n",
      "        [-0.0572,  0.1295, -0.1692, -0.1017]])\n",
      "b1:\n",
      " tensor([-0.5022, -0.3446, -0.1910])\n",
      "W2:\n",
      " tensor([[ 0.0217,  0.2695,  0.3465],\n",
      "        [-0.3883, -0.2426,  0.2103]])\n",
      "b2:\n",
      " tensor([0.6073, 0.0631])\n",
      "==================================================\n",
      "\n",
      "=== PyTorch vs Manual 최종 파라미터 차이(절댓값 최댓값) ===\n",
      "fc1.weight 차이: 0.0\n",
      "fc1.bias   차이: 0.0\n",
      "fc2.weight 차이: 0.0\n",
      "fc2.bias   차이: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================================\n",
    "# 4. 위에서 복사해둔 '초기' 파라미터로 수동 연산\n",
    "# ===============================================\n",
    "W1 = init_W1.clone()\n",
    "b1 = init_b1.clone()\n",
    "W2 = init_W2.clone()\n",
    "b2 = init_b2.clone()\n",
    "\n",
    "# -- 수동 forward 구현 --\n",
    "def forward_manual(x, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    x: (2,4)\n",
    "    W1: (3,4)\n",
    "    b1: (3,)\n",
    "    W2: (2,3)\n",
    "    b2: (2,)\n",
    "    \"\"\"\n",
    "    # fc1: z1 = x.mm(W1^T) + b1\n",
    "    #      => shape(2,3)\n",
    "    z1 = x.mm(W1.t()) + b1\n",
    "    # relu\n",
    "    a1 = torch.relu(z1)\n",
    "    # fc2: z2 = a1.mm(W2^T) + b2\n",
    "    #      => shape(2,2)\n",
    "    z2 = a1.mm(W2.t()) + b2\n",
    "    \n",
    "    return z2, a1, z1\n",
    "\n",
    "# -- 수동 backward 구현 --\n",
    "def backward_manual(x, y, out, a1, z1, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    x:   (2,4)\n",
    "    y:   (2,2)\n",
    "    out: (2,2)  (forward_manual 결과)\n",
    "    a1:  (2,3)  (middle layer activation)\n",
    "    z1:  (2,3)  (middle layer linear output)\n",
    "    \n",
    "    W1:  (3,4)\n",
    "    b1:  (3,)\n",
    "    W2:  (2,3)\n",
    "    b2:  (2,)\n",
    "    \"\"\"\n",
    "    # 1) loss = mean((out - y)^2)\n",
    "    #    shape (2,2) => 총 4개 요소\n",
    "    #    dLoss/dOut = (out - y) * (2 / 4) = (out - y) / 2\n",
    "    loss = ((out - y) ** 2).mean()\n",
    "    dLoss_dOut = (out - y) / 2.0  # shape (2,2)\n",
    "\n",
    "    # 2) fc2: z2 = a1.mm(W2^T) + b2\n",
    "    #    => out = z2\n",
    "    #    dLoss/dW2 = dLoss/dz2 . a1 (broadcast)\n",
    "    #       (2,2).T mm (2,3) => (2,3)\n",
    "    #    dLoss/db2 = sum(dLoss/dz2) over batch\n",
    "    #       (2,2) => sum along dim=0 => (2,)\n",
    "    dW2 = dLoss_dOut.t().mm(a1)     # shape (2,3)\n",
    "    db2 = dLoss_dOut.sum(dim=0)     # shape (2,)\n",
    "\n",
    "    # 3) middle layer (ReLU)\n",
    "    #    dLoss/d(a1) = dLoss/dOut mm W2\n",
    "    #       shape: (2,2) mm (2,3) => (2,3)\n",
    "    dLoss_da1 = dLoss_dOut.mm(W2)   # (2,3)\n",
    "    relu_mask = (z1 > 0).float()    # (2,3)\n",
    "    dLoss_dz1 = dLoss_da1 * relu_mask  # (2,3)\n",
    "\n",
    "    # 4) fc1: z1 = x.mm(W1^T) + b1\n",
    "    #    => dW1 = (dLoss_dz1).T mm x\n",
    "    #       => shape (3,4)\n",
    "    #    => db1 = sum(dLoss_dz1, dim=0)\n",
    "    #       => shape (3,)\n",
    "    dW1 = dLoss_dz1.t().mm(x)       # (3,4)\n",
    "    db1 = dLoss_dz1.sum(dim=0)      # (3,)\n",
    "\n",
    "    return loss, dW1, db1, dW2, db2\n",
    "\n",
    "# -- 수동 forward & backward & update(1 step) --\n",
    "out, a1, z1 = forward_manual(x, W1, b1, W2, b2)\n",
    "loss_manual, gW1, gb1, gW2, gb2 = backward_manual(x, y, out, a1, z1, W1, b1, W2, b2)\n",
    "\n",
    "lr = 0.1\n",
    "W1 = W1 - lr * gW1\n",
    "b1 = b1 - lr * gb1\n",
    "W2 = W2 - lr * gW2\n",
    "b2 = b2 - lr * gb2\n",
    "\n",
    "print(\"=== 수동 연산으로 업데이트된 최종 파라미터 ===\")\n",
    "print(\"W1:\\n\", W1)\n",
    "print(\"b1:\\n\", b1)\n",
    "print(\"W2:\\n\", W2)\n",
    "print(\"b2:\\n\", b2)\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. PyTorch vs 수동 파라미터 차이 비교\n",
    "# ==========================================\n",
    "diff_w1 = (model.fc1.weight.data - W1).abs().max().item()\n",
    "diff_b1 = (model.fc1.bias.data   - b1).abs().max().item()\n",
    "diff_w2 = (model.fc2.weight.data - W2).abs().max().item()\n",
    "diff_b2 = (model.fc2.bias.data   - b2).abs().max().item()\n",
    "\n",
    "print(\"=== PyTorch vs Manual 최종 파라미터 차이(절댓값 최댓값) ===\")\n",
    "print(\"fc1.weight 차이:\", diff_w1)\n",
    "print(\"fc1.bias   차이:\", diff_b1)\n",
    "print(\"fc2.weight 차이:\", diff_w2)\n",
    "print(\"fc2.bias   차이:\", diff_b2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
